{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "057c3d6c-5886-44fe-b02f-1a7f19e6a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (2025.11.12)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bef3bb53-7523-40d5-aed6-29b30bcbdc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2406bece-861a-4e14-ba6f-9cd093338e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, UnidentifiedImageError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7333dc5b-cab7-4c71-8f50-0b0dd1f725a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_loader(path):\n",
    "    try:\n",
    "        # Try to open with PIL\n",
    "        return Image.open(path).convert(\"RGB\")\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"Skipping bad file: {path}\")\n",
    "        # Return a dummy image (black 224x224) so DataLoader doesn’t crash\n",
    "        return Image.new(\"RGB\", (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eafc189b-bc9e-42fb-8217-b8f51677124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms (resize, tensor conversion, normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # resize to fit CNN input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # grayscale normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e8fda74-68ac-4364-b884-aadf4aba83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=\"data/rvl_cdip\", loader=safe_loader,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c1fa923-64de-449f-a6a6-7f293cdcc3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39997\n",
      "['advertisement', 'budget', 'email', 'file_folder', 'form', 'handwritten', 'invoice', 'letter', 'memo', 'news_article', 'presentation', 'questionnaire', 'resume', 'scientific_publication', 'scientific_report', 'specification']\n",
      "{'advertisement': 0, 'budget': 1, 'email': 2, 'file_folder': 3, 'form': 4, 'handwritten': 5, 'invoice': 6, 'letter': 7, 'memo': 8, 'news_article': 9, 'presentation': 10, 'questionnaire': 11, 'resume': 12, 'scientific_publication': 13, 'scientific_report': 14, 'specification': 15}\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))          # total number of images\n",
    "print(dataset.classes)       # list of class names\n",
    "print(dataset.class_to_idx)  # mapping class → label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a2fb84b-3ef0-4684-b36e-bd57f20229fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))   # 70% for training\n",
    "test_size = int(0.2 * len(dataset))  # 20% for testing\n",
    "val_size = len(dataset)-train_size-test_size # 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8aac7354-58ea-4a6b-8abe-55ace2193df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fea15b75-bce7-4a46-8c4a-f201a716e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader   = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5fd092ba-5dcf-460e-9ab7-d042ab3780a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41264f73-55de-40d1-8ea5-7f47b8173b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "model = models.resnet18(weights=None)  # no auto-download\n",
    "state_dict = torch.load(\"resnet18-5c106cde.pth\",weights_only=False)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30b5862c-de5a-4f46-b697-ed23854883e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the final layer\n",
    "import torch.nn as nn\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e118065-344b-4c4a-82f2-211d4cb2b5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c5c13b2-1db7-4559-9cb9-c21ca1e7dba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2349\n",
      "Validation Accuracy: 82.18%\n",
      "Best model updated and saved!\n",
      "Epoch 2, Loss: 0.1760\n",
      "Validation Accuracy: 81.20%\n",
      "Epoch 3, Loss: 0.1730\n",
      "Validation Accuracy: 81.40%\n",
      "Epoch 4, Loss: 0.2619\n",
      "Validation Accuracy: 80.88%\n",
      "Epoch 5, Loss: 0.1375\n",
      "Validation Accuracy: 81.63%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Make sure the checkpoints directory exists\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(5):  # example: 5 epochs\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # --- Checkpointing ---\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"accuracy\": acc,\n",
    "        \"loss\": loss.item()\n",
    "    }\n",
    "    torch.save(checkpoint, f\"checkpoints/epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # Save best model separately\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"checkpoints/best_model.pth\")\n",
    "        print(\"Best model updated and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c73152d-ab29-4c05-bcec-0eb2e44f6e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping bad file: data/rvl_cdip/scientific_publication/2500126531_2500126536.tif\n",
      "Test Accuracy: 81.32%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "466e941f-f32a-4fe8-bcfd-3bc7c9407a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping bad file: data/rvl_cdip/scientific_publication/2500126531_2500126536.tif\n",
      "Test Accuracy: 81.32%\n"
     ]
    }
   ],
   "source": [
    "# Step 7 \n",
    "# Inference\n",
    "# Load best checkpoint\n",
    "state_dict = torch.load(\"checkpoints/best_model.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "test_data = torch.randn(10, 3,32,32)  # 10 new samples\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed67c6b-2662-48bc-bbe0-65f9f5b4ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8\n",
    "# Resume the checkpoint\n",
    "checkpoint = torch.load('checkpoints/best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimiser.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "# Continue training\n",
    "for epoch in range(start_epoch, num_epochs + 5):\n",
    "    # training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_data,batch_target in train_dataloader:\n",
    "        output = model(batch_data)\n",
    "        loss = criterion(output,batch_target) # Compute loss\n",
    "        optimiser.zero_grad() # Zero gradients from previous iteration\n",
    "        loss.backward() # calcluate new gradients via back propagation\n",
    "        optimiser.step() # updates gradients\n",
    "        train_loss+=loss.item()\n",
    "        \n",
    "    train_loss/=len(train_dataloader)      \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data,batch_target in val_dataloader:\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output,batch_target)\n",
    "            val_loss+=loss.item()\n",
    "        val_loss/=len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimiser.state_dict(),\n",
    "            'loss': val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, 'checkpoints/best_model.pt')\n",
    "        print(f\"  ✓ Best model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
